package core

import (
	"context"
	_ "embed"
	"encoding/json"
	"fmt"
	"github.com/google/uuid"
	plan "github.com/viant/agently/genai/agent/plan"
	"github.com/viant/agently/genai/llm"
	"github.com/viant/agently/genai/llm/provider/base"
	"github.com/viant/agently/genai/memory"
	"strings"
)

// ToolKey uniquely identifies a tool call by its name and canonicalised
// arguments JSON.  Hashable so it can be used as a map key.
type ToolKey struct {
	Name string
	Args string
}

// Plan represents a structured Plan generated by the LLM.
//
//go:embed prompt/plan_prompt.vm
var planPromptTemplate string

// PlanInput defines input for Plan generation or finalization.
type PlanInput struct {
	Query      string               `json:"query"`
	Context    string               `json:"context,omitempty"`
	Model      string               `json:"model,omitempty"`
	ModelMatch llm.ModelPreferences `json:"modelMatch,omitempty"`
	Tools      []string             `json:"tools,omitempty"`     // available tools for selection
	PromptURI  string               `json:"promptURI,omitempty"` // optional custom prompt for Plan generation
	//Loopback parameters
	Results       []plan.Result    `json:"results,omitempty"`    // structured step results for finalization
	Transcript    []memory.Message `json:"transcript,omitempty"` // transcript of the conversation with the LLM`
	ResultSummary string           `json:"resultSummary,omitempty"`
}

// PlanOutput defines Plan or Answer output.
type PlanOutput struct {
	Plan          *plan.Plan       `json:"plan,omitempty"`
	Answer        string           `json:"answer,omitempty"`
	Results       []plan.Result    `json:"results,omitempty"`
	Transcript    []memory.Message `json:"transcript,omitempty"` // transcript of the conversation with the LLM`
	ResultSummary string           `json:"resultSummary,omitempty"`
}

// Plan calls the LLM to generate a YAML-encoded Plan
func (s *Service) plan(ctx context.Context, in, out interface{}) error {
	input := in.(*PlanInput)
	output := out.(*PlanOutput)

	err := s.Plan(ctx, input, output)
	return err
}

// Plan generates a structured Plan based on the provided input.
func (s *Service) Plan(ctx context.Context, input *PlanInput, output *PlanOutput) error {
	output.Results = input.Results
	output.Transcript = input.Transcript
	modelName := input.Model
	if modelName == "" {
		modelName = s.defaultModel
	}
	tools, err := s.registry.MustHaveTools(input.Tools)
	if err != nil {
		return err
	}

	promptTemplate := ""
	if input.PromptURI != "" {
		data, err := s.fs.DownloadWithURL(ctx, input.PromptURI)
		if err != nil {
			return err
		}
		promptTemplate = string(data)
	}
	if promptTemplate == "" {
		promptTemplate = planPromptTemplate
	}
	genOutput := &GenerateOutput{}
	planResult, err := s.GeneratePlan(ctx, modelName, promptTemplate, input, tools, genOutput)
	if err != nil {
		return err
	}
	if planResult.Elicitation.IsEmpty() {
		planResult.Elicitation = nil
	}

	if output.Plan.IsEmpty() {
		for _, choice := range genOutput.Response.Choices {
			if choice.Message.Role == llm.RoleAssistant {
				if planResult.Elicitation != nil && json.Valid([]byte(choice.Message.Content)) {
					continue //that was elicitation content
				}

				output.Transcript = append(output.Transcript, memory.Message{Role: string(choice.Message.Role), Content: choice.Message.Content})
			}
		}
	}
	for _, transcript := range output.Transcript {
		output.Answer += transcript.Content + "\n"
	}
	RefinePlan(input.Results, planResult, output.Answer)
	output.Plan = planResult
	output.Results = input.Results
	return nil
}

func RefinePlan(prior []plan.Result, p *plan.Plan, _ string) {
	if len(p.Steps) == 0 {
		return
	}

	guard := NewDuplicateGuard(prior) // detects cross-iteration loops

	seenThisPlan := map[ToolKey]struct{}{}
	filtered := make(plan.Steps, 0, len(p.Steps))

	for _, st := range p.Steps {
		if st.Type != "tool" {
			filtered = append(filtered, st)
			continue
		}

		// Intra-plan duplicate removal (adjacent or not)
		tk := ToolKey{st.Name, CanonicalArgs(st.Args)}
		if _, ok := seenThisPlan[tk]; ok {
			continue // skip duplicate inside the same plan
		}

		// Cross-iteration guard â€“ block if loop pattern detected
		if block, _ := guard.ShouldBlock(st.Name, st.Args); block {
			continue // remove before execution to end loop early
		}

		seenThisPlan[tk] = struct{}{}
		filtered = append(filtered, st)
	}

	p.Steps = filtered
}

// GeneratePlan invokes the LLM to produce or refine a plan based on the template and bind variables.
// It returns a Plan if parsing succeeded, otherwise returns the answer text.
func (s *Service) GeneratePlan(ctx context.Context, modelName, promptTemplate string, input *PlanInput, tools []llm.Tool, genOutput *GenerateOutput) (*plan.Plan, error) {

	if len(input.ModelMatch.Hints) == 0 && modelName != "" {
		input.ModelMatch.Hints = append(input.ModelMatch.Hints, modelName)
	}
	if model := s.modelMatcher.Best(&input.ModelMatch); model != "" {
		modelName = model
	}

	bind := map[string]interface{}{
		"Query":         input.Query,
		"Context":       input.Context,
		"Results":       input.Results,
		"Tools":         input.Tools,
		"ResultSummary": input.ResultSummary,
		"CanUseTools":   false,
	}
	if model, _ := s.llmFinder.Find(ctx, modelName); model != nil {
		bind["Model"] = model
		bind["CanUseTools"] = model.Implements(base.CanUseTools)
	}

	s.enureResultCallID(input)

	genInput := &GenerateInput{
		Model:    modelName,
		Template: promptTemplate,
		Bind:     bind,
		Tools:    tools,
	}

	if len(input.Results) > 0 {
		for _, result := range input.Results {
			var toolCalls []llm.ToolCall
			var toolResultMessage []llm.Message
			toolCall := llm.NewToolCall(result.ID, result.Name, result.Args)
			toolCalls = append(toolCalls, toolCall)
			callResult := result.Result
			if result.Error != "" {
				callResult = "Error:" + result.Error
			}
			toolResultMessage = append(toolResultMessage, llm.NewToolResultMessage(toolCall, callResult))
			genInput.Message = append(genInput.Message, llm.NewAssistantMessageWithToolCalls(toolCalls...))
			genInput.Message = append(genInput.Message, toolResultMessage...)
		}
	}

	aPlan, err := s.generatePlan(ctx, genInput, genOutput)
	return aPlan, err
}

func (s *Service) enureResultCallID(input *PlanInput) {
	for i := range input.Results {
		result := input.Results[i]
		if result.ID == "" {
			result.ID = uuid.New().String()
		}
	}
}

func (s *Service) generatePlan(ctx context.Context, genInput *GenerateInput, genOutput *GenerateOutput) (*plan.Plan, error) {
	if err := s.Generate(ctx, genInput, genOutput); err != nil {
		return nil, err
	}
	aPlan := plan.New()

	// Handle nested function calls (tool calls) in initial plan response.
	if len(genOutput.Response.Choices) > 0 {
		for j := range genOutput.Response.Choices {
			choice := genOutput.Response.Choices[j]
			if len(choice.Message.ToolCalls) > 0 {
				steps := make(plan.Steps, 0, len(choice.Message.ToolCalls))
				for _, tc := range choice.Message.ToolCalls {
					name := tc.Name
					args := tc.Arguments
					if name == "" && tc.Function.Name != "" {
						name = tc.Function.Name
					}
					if args == nil && tc.Function.Arguments != "" {
						var parsed map[string]interface{}
						if err := json.Unmarshal([]byte(tc.Function.Arguments), &parsed); err == nil {
							args = parsed
						}
					}
					steps = append(steps, plan.Step{
						ID:     uuid.New().String(),
						Type:   "tool",
						Name:   name,
						Args:   args,
						Reason: choice.Message.Content,
					})
					data, _ := json.Marshal(args)
					fmt.Printf("%s -> %s\n", name, string(data))
				}
				aPlan.Steps = append(aPlan.Steps, steps...)

			}
		}
		return aPlan, nil
	}
	var err error
	if strings.Contains(genOutput.Content, `"tool"`) {
		err = EnsureJSONResponse(ctx, genOutput.Content, aPlan)
	}
	if strings.Contains(genOutput.Content, `"elicitation"`) {
		aPlan.Elicitation = &plan.Elicitation{}
		_ = EnsureJSONResponse(ctx, genOutput.Content, aPlan.Elicitation)
		if aPlan.Elicitation.IsEmpty() {
			aPlan.Elicitation = nil
		}
	}
	aPlan.Steps.EnsureID()

	// ------------------------------------------------------------------
	// Enrich step reason with leading narrative text when Reason is empty.
	// Many LLMs output helpful prose immediately before the JSON plan. We
	// capture that prose (text until the opening `{` or fenced ```json block)
	// and store it as the Reason of the first step when the JSON itself does
	// not already carry one.
	// ------------------------------------------------------------------
	if len(aPlan.Steps) > 0 && strings.TrimSpace(aPlan.Steps[0].Reason) == "" {
		prefix := genOutput.Content
		if idx := strings.Index(prefix, "```json"); idx != -1 {
			prefix = prefix[:idx]
		} else if idx := strings.Index(prefix, "{"); idx != -1 {
			prefix = prefix[:idx]
		}
		prefix = strings.TrimSpace(prefix)
		if prefix != "" {
			aPlan.Steps[0].Reason = prefix
		}
	}
	return aPlan, err
}
